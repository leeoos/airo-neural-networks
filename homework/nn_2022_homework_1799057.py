# -*- coding: utf-8 -*-
"""NN_2022_Homework_1799057

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Do32Kifkw06z4YkgVN3kd6Aw4-7raQT

# Neural Networks
## Homework 1: Implementing advanced activation functions

**Name**: *\<insert your name here\>*

**Matricola**: *\<insert your ID here\>*

Upload the completed notebook **one week before the exam date of your choice** on the Google Classrom page.
"""

import torch
import torch.nn as nn

"""### Objective

The purpose of this homework is to implement a new layer inside PyTorch, by properly extending the `nn.Module` object. **Before proceeding**, carefully read the following documentation:

+ [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=module#torch.nn.Module)
+ [PyTorch: Custom Module](https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html)

You can also (optionally) learn more about activation functions by reading the survey in [1].

### Introduction: families of activation functions

In the course up to now, we have seen the application of the sigmoid, the softmax, and the ReLU. However, many additional activation functions exist [1], with varying strengths and drawbacks.

Several of them are designed as variants of the ReLU. The **S-Shaped ReLU** (SReLU) [2] is defined as:

$$
\phi(s) = \begin{cases} t^r + a^r(s - t^r) & \text{ if } s > t^r \\ s & \text{ if } t^r > s > t^l \\ t^l + a^l(s - t^l) & \text{ if } s < t^l \end{cases} \,.
$$

The four parameters $t^r, a^r, t^l, a^l$ are trained via back-propagation, and they are **different for each unit in the layer**.

### Exercise 1: implementing an activation function (1 point)

Let us start with the simpler **[exponential linear squashing](https://paperswithcode.com/method/elish)** (ELiSH) activation function:

$$
\phi(x) = \begin{cases} \sigma(x)x & \text{ if } x \ge 0 \\ \frac{\exp(x) - 1}{1 + \exp(-x)} & \text{ otherwise} \end{cases}
$$

**Exercise 1**: complete the following stub.
"""

def elish(x):
  # x is a generic torch.Tensor, and this function must compute the ELiSH activation function.
  # raise NotImplementedError # TODO: substitute this with your code

  # Alternative: use torch.sigmioid
  sigmoid = torch.sigmoid(x)
  elish_sig = sigmoid * x
  elish_exp = sigmoid * (torch.exp(x) -1)


  # Build tensor of booleans to check the conndiction
  x_gt_eq_zero = x >= 0

  # ELisH implementation with torch condictional
  output = torch.where(x_gt_eq_zero, elish_sig, elish_exp)

  return output

"""**Hints for a correct implementation**:

1. There are several ways of implementing an if/else operation like the one above in PyTorch. In general, the simplest implementation of "*if a then b, else c*" is `torch.where(a, b, c)` (see the documentation for [torch.where](https://pytorch.org/docs/stable/generated/torch.where.html)). Any working variant is accepted here.

Here is a simple sanity check for the correct implementation:
"""

elish(torch.FloatTensor([[0.2, -0.4]])) # Should be approximately [[0.11, -0.13]]

"""### Exercise 2: some visualization experiments (1 point)

**Exercise 2.1**: plot the ELiSH function in [-5, +5].
"""

import matplotlib.pyplot as plt
# TODO: plot the function

# Points interval definition
x = torch.linspace(-5, 5, 100)
y = elish(x)
x.requires_grad = True # enableling gradient tracking for later computation

# Figure definition
plt.title('ELiSH Activation Function')
plt.xlabel('x values')
plt.ylabel('ELiSH(x)')
plt.grid(True)

# Calls to detach and numpy are used to convert the PyTorch tensors to NumPy arrays for plotting.
plt.plot(x.detach().numpy(), y.detach().numpy())
plt.show()

"""**Exercise 2.2**: using the utilities from `torch.autograd` ([torch.autograd](https://pytorch.org/docs/stable/autograd.html)), **compute and plot** the derivative of ELiSH using automatic differentiation."""

# TODO: plot the gradient of the ELiSH function

# Since elish(x) return a tesor of the ELiSH activation function values at different x points,
# summing it will reduce it to a scalar value.
y = elish(x).sum()

# Compute gradients with respect to x.
gradient = torch.autograd.grad(y, x)    # returns a tuple of gradients, one for each input.
gradient = gradient[0]  # since we only have one element


# Figure definition
plt.title('ELiSH Gradien')
plt.xlabel('x values')
plt.ylabel('grad(ELiSH(x))')
plt.grid(True)

plt.plot(x.detach().numpy(), gradient.detach().numpy())
plt.show()

"""**Exercise 2.3 (sanity check)**: build a model using the previously defined activation function, and test it on a random mini-batch of data:"""

# TODO: complete the definition of the model
W = torch.rand((10, 1))
B = torch.rand((1, 1))
model = lambda X: elish(X@W + B)

print(model(torch.randn((5, 10))))

"""## Exercise 3: implementing a trainable activation function (2 points)

**Exercise 3:** define a `torch.nn.Module` implementing the SReLU.

**Hints for a correct implementation**:
* The layer should **only** implement the activation function. Ideally, it will always be used in combination with a fully-connected layer with no activation function.
* Think carefully about how you want to initialize the parameters.
"""

class SReLU(nn.Module):
  """
  S-shaped Rectified Linear Unit.
  It follows:
    f(x) = b^r + w^r(x - b^r)   for x >= b^r,
    f(x) = x                    for b^r > x > b^l`,
    f(x) = b^l + w^l(x - b^l)   for x <= b^l.
  """

  def __init__(self, units):
    # raise NotImplementedError # TODO: implement

    # Call super constructor to inherit all the methods and properties from nn.Module:
    super(SReLU, self).__init__()
    self.units = units

    # Weights and Biases initialization for each unit in the layer
    self.w_l = nn.Parameter(torch.zeros(units), requires_grad=True)
    self.w_r = nn.Parameter(torch.rand(units), requires_grad=True)
    self.b_l = nn.Parameter(torch.ones(units), requires_grad=True)
    self.b_r = nn.Parameter(torch.rand(units), requires_grad=True)

    # Parameters registration
    # self.register_parameter('w_l', self.w_l)
    # self.register_parameter('w_r', self.w_r)
    # self.register_parameter('b_l', self.b_l)
    # self.register_parameter('b_r', self.b_r)

    # self.total_parameters = int(len(self.w_l)) + int(len(self.w_l))
    #                         + int(len(self.b_l)) + int(len(self.b_l))

  def forward(self, inputs):
    # raise NotImplementedError # TODO: implement

    # Set the output to be equal to the input as if b_l < inputs < b_r
    output = inputs

    # If input >= b^r then apply the right part of SReLU otherwise keep the output as it is
    right_input = inputs >= self.b_r
    output = torch.where(right_input, (self.b_r + self.w_r*(inputs - self.b_r)), output)

    # If input <= b^l then apply the left part of SReLU otherwise keep the output as it is
    left_input = inputs <= self.b_l
    output = torch.where(right_input, (self.b_l + self.w_l*(inputs - self.b_l)), output)

    #print(output)

    return output

"""As a sanity check, initialize a SReLU layer and count the number of parameters:"""

# Initialize the layer
layer = SReLU(2)

# Count the parameters
print(sum(_.numel() for _ in layer.parameters())) # Should print 8!

"""## Exercise 4: training a model with trainable activation functions (1 point)

We will use the following dataset from TensorFlow Datasets:
https://www.tensorflow.org/datasets/catalog/german_credit_numeric
"""

import tensorflow_datasets as tfds
train_data = tfds.load('german_credit_numeric', split='train[:75%]', as_supervised=True)
test_data = tfds.load('german_credit_numeric', split='train[75%:]', as_supervised=True)

Xtrain, ytrain = train_data.batch(5000).get_single_element()
Xtrain, ytrain = Xtrain.numpy(), ytrain.numpy()

"""**Exercise 4**: write a `nn.Module` using the previous `SReLU`, and train it on the german_credit_numeric dataset."""

from torch.utils.data import Dataset, TensorDataset
from sklearn.preprocessing import StandardScaler, normalize

# Check on the tensors shape to be on the safe side
print(f'Inputs shape: {Xtrain.shape}')
print(f'Labels shape: {ytrain.shape}')

# Normalization with zero meand and unitary standard deviation
Xtrain = StandardScaler().fit_transform(Xtrain)
#Xtrain = normalize(Xtrain)

# Definition of torch Dataset
class GermanCreditDataset(Dataset):

  def __init__(self, Xtrain, ytrain):
    super(GermanCreditDataset, self).__init__()
    self.Xtrain = Xtrain
    self.ytrain = ytrain
    self.input_len = self.Xtrain.shape[1]

  def __getitem__(self, index):

    # Convert the input and labels to pytorch tensors
    X_train = torch.Tensor(self.Xtrain)
    y_train = torch.Tensor(self.ytrain)

    return X_train, y_train

  def __len__(self):
    return len(self.Xtrain)


# Create a pyTorch dataLoader for batch processing
torch_dataset = GermanCreditDataset(Xtrain, ytrain)
train_dataloader = torch.utils.data.DataLoader(torch_dataset, batch_size=64, shuffle=True)

from tensorflow_datasets.core.dataset_builder import dataset_metadata
# TODO: define and train a model

class HwModel(nn.Module):

  def __init__(self):
    super(HwModel, self).__init__()

    # model architecture
    self.fc1 = nn.Linear(24, 10)
    self.srelu1 = SReLU(10)
    self.fc2 = nn.Linear(10,4)
    self.srelu2 = SReLU(4)
    self.fc3 = nn.Linear(4,1)

  def forward(self, input):

    # Forward process
    output = self.fc1(input)
    output = self.srelu1(output)
    output = self.fc2(output)
    output = self.srelu2(output)
    output = self.fc3(output)

    return output

model = HwModel()

# Training

# Set device for training
device = 'cuda' if torch.cuda.is_available() else 'cpu'
#print(device) # to be on the safe side

model = model.to(device)
loss_fn = nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(model.parameters())

EPOCHS = 10

for epoch in range(EPOCHS):

  sum_loss = 0.
  avg_loss = 0.

  for i, batch in enumerate(train_dataloader):

    # Every data instance is an (input, label) pair
    inputs, labels = batch

    inputs = inputs.to(device)
    labels = labels.to(device)

    # Zero your gradients for every batch!
    optimizer.zero_grad()

    # Make predictions for this batch
    outputs = model(inputs)

    # reshape the model prediction to remove unwanted 3-rd dimention
    outputs = outputs.reshape(outputs.shape[0], outputs.shape[1])
    # print(outputs.shape)
    # print(outputs)
    # print(labels.shape)
    # print(labels)

    # Compute the loss and its gradients
    loss = loss_fn(outputs, labels)
    loss.backward()

    # Adjust learning weights
    optimizer.step()

    # Accumulate the losses
    sum_loss += loss.item()

    # Print average loss for the epoch
    num_of_batches = len(train_dataloader)
    if i % num_of_batches == num_of_batches -1:
        avg_loss = sum_loss / num_of_batches # loss per batch
        print(f'  Epoch {epoch+1} loss: {avg_loss}')
        sum_loss = 0.

"""**Optionally**, you can plot the distribution (histogram) of the parameters after training."""

# TODO: plot the histogram
import numpy as np

# Obtain the parameter values from the trained model
parameters = []
for param in model.parameters():
    print(param)
    parameters.extend(param.flatten().detach().numpy())
parameters = np.array(parameters)

# Plot the histogram
plt.hist(parameters, bins=10)
plt.xlabel('Parameter Value')
plt.ylabel('Frequency')
plt.title('Parameter Distribution')
plt.show()

"""## Final checklist
1. Carefully check all code. Insert comments when needed. Search for "TODO" to see if you forgot something.
2. Run everything one final time. *Please do not send us notebooks with errors or cells that are not working.*
3. Upload the completed notebook **one week before the exam date of your choice** on the Google Classrom page.

### References

[1] Apicella, A., Donnarumma, F., IsgrÃ², F. and Prevete, R., 2021. [A survey on modern trainable activation functions](https://arxiv.org/abs/2005.00817).

[2] Jin, X., Xu, C., Feng, J., Wei, Y., Xiong, J. and Yan, S., 2016. [Deep learning with s-shaped rectified linear activation units](https://arxiv.org/abs/1512.07030). In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 30, No. 1).
"""